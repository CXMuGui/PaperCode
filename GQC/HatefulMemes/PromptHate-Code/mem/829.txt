DATASET : mem
FEW_SHOT : False
FINE_GRIND : False
NUM_SHOTS : 16
MODEL : pbm
UNIMODAL : False
DATA : /212023085404022/workspace/PaperCode/HatefulMemes/data
CAPTION_PATH : /212023085404022/workspace/PaperCode/HatefulMemes/caption
RESULT : ./result
FEAT_DIM : 2048
CLIP_DIM : 512
BERT_DIM : 768
ROBERTA_DIM : 1024
NUM_FOLD : 5
EMB_DIM : 300
NUM_LABELS : 2
POS_WORD : good
NEG_WORD : bad
DEM_SAMP : False
SIM_RATE : 0.5
IMG_RATE : 0.5
TEXT_RATE : 0.5
CLIP_CLEAN : False
MULTI_QUERY : True
NUM_QUERIES : 4
EMB_DROPOUT : 0.0
FC_DROPOUT : 0.4
WEIGHT_DECAY : 0.01
LR_RATE : 1.3e-05
EPS : 1e-08
BATCH_SIZE : 16
FIX_LAYERS : 2
MID_DIM : 512
NUM_HIDDEN : 512
LENGTH : 64
TOTAL_LENGTH : 256
PREFIX_LENGTH : 10
NUM_SAMPLE : 1
NUM_LAYER : 8
MODEL_NAME : roberta-large
PRETRAIN_DATA : conceptual
IMG_VERSION : clean
MAPPING_TYPE : transformer
ADD_ENT : True
ADD_DEM : True
DEBUG : False
SAVE : False
SAVE_NUM : 829
EPOCHS : 10
SEED : 1119
CUDA_DEVICE : 1
WARM_UP : 2000
TRANS_LAYER : 1
NUM_HEAD : 8
Length of training set: 8500, length of testing set: 500
Epoch 0
	train_loss: 924.29, accuracy: 66.29
	evaluation auc: 68.84, accuracy: 63.40
Epoch 1
	train_loss: 535.26, accuracy: 76.87
	evaluation auc: 74.53, accuracy: 68.20
Epoch 2
	train_loss: 432.45, accuracy: 82.22
	evaluation auc: 78.22, accuracy: 71.20
Epoch 3
	train_loss: 344.01, accuracy: 86.64
	evaluation auc: 79.88, accuracy: 63.80
Epoch 4
	train_loss: 251.61, accuracy: 90.88
	evaluation auc: 80.96, accuracy: 72.60
Epoch 5
	train_loss: 175.68, accuracy: 93.67
	evaluation auc: 82.51, accuracy: 70.40
Epoch 6
	train_loss: 106.88, accuracy: 96.40
	evaluation auc: 80.38, accuracy: 69.20
Epoch 7
	train_loss: 74.66, accuracy: 97.40
	evaluation auc: 81.83, accuracy: 72.80
Epoch 8
	train_loss: 48.95, accuracy: 98.33
	evaluation auc: 81.51, accuracy: 71.20
Epoch 9
	train_loss: 27.15, accuracy: 99.06
	evaluation auc: 82.09, accuracy: 73.20
Maximum epoch: 9
	evaluation auc: 82.09, accuracy: 73.20
